#!/usr/bin/env python3
"""
Main benchmark runner script for RequestX performance testing.

This script runs comprehensive benchmarks comparing RequestX against other
HTTP libraries and exports results in multiple formats including CSV, JSON,
and OpenTelemetry.
"""

import os
import sys
import argparse
import json
from typing import List, Dict, Any
from datetime import datetime

# Add the parent directory to sys.path to import requestx
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', 'python'))

import requestx
from requestx import BenchmarkRunner, BenchmarkConfig, BenchmarkResult
from requestx.benchmark import RequestXBenchmarker
from requestx.exporter import export_to_grafana_cloud, export_to_otlp
from requestx.profiler import profile_context, PerformanceMetrics
from requestx.benchmark import BenchmarkerSync, BenchmarkerAsync


class RequestsBenchmarker(BenchmarkerSync):
    """Benchmarker for requests library."""
    
    def __init__(self):
        super().__init__('requests')
    
    def setup(self):
        import requests
        self.session = requests.Session()
    
    def teardown(self):
        if self.session:
            self.session.close()
    
    def make_request(self, url: str, method: str = 'GET', **kwargs) -> bool:
        try:
            response = self.session.request(method, url, **kwargs)
            return 200 <= response.status_code < 400
        except Exception:
            return False


class HttpxBenchmarker(BenchmarkerSync):
    """Benchmarker for httpx library."""
    
    def __init__(self):
        super().__init__('httpx')
    
    def setup(self):
        import httpx
        self.session = httpx.Client()
    
    def teardown(self):
        if self.session:
            self.session.close()
    
    def make_request(self, url: str, method: str = 'GET', **kwargs) -> bool:
        try:
            response = self.session.request(method, url, **kwargs)
            return 200 <= response.status_code < 400
        except Exception:
            return False


class AiohttpBenchmarker(BenchmarkerAsync):
    """Benchmarker for aiohttp library."""
    
    def __init__(self):
        super().__init__('aiohttp')
    
    def setup(self):
        # Session will be created in async context
        self.session = None
    
    def teardown(self):
        # Sessions are managed per request
        pass
    
    async def make_async_request(self, url: str, method: str = 'GET', **kwargs) -> bool:
        """Make async request using aiohttp."""
        try:
            import aiohttp
            
            # Create session if not already created
            session = self.session
            if session is None:
                session = aiohttp.ClientSession()
                session_created_here = True
            else:
                session_created_here = False
            
            try:
                async with session.request(method, url, **kwargs) as response:
                    return 200 <= response.status < 400
            finally:
                if session_created_here:
                    await session.close()
        except Exception:
            return False


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Run RequestX performance benchmarks",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage - run quick benchmarks with default settings
  ./requestx-benchmark --quick
  
  # Full benchmarks with custom concurrency and request count
  ./requestx-benchmark --concurrency 1,10,100,1000 --requests 500
  
  # Test specific HTTP methods only
  ./requestx-benchmark --methods GET,POST --sizes small,medium
  
  # Use custom test server (default is https://httpbin.org)
  ./requestx-benchmark --host http://localhost:8080 --quick
  ./requestx-benchmark --host https://api.example.com --methods GET
  
  # Compare specific libraries
  ./requestx-benchmark --libraries requestx,requests --quick
  ./requestx-benchmark --libraries requestx --verbose
  
  # Custom output and reporting
  ./requestx-benchmark --output-dir ./my_results --verbose
  ./requestx-benchmark --no-csv --output-dir ./json_only
  
  # Export results to monitoring systems
  ./requestx-benchmark --grafana-cloud --quick
  ./requestx-benchmark --otlp-endpoint http://localhost:4317
  ./requestx-benchmark --otlp-endpoint http://jaeger:14268/api/traces --otlp-headers '{"Authorization":"Bearer token"}'
  
  # Performance testing scenarios
  ./requestx-benchmark --concurrency 1,50,100,500 --requests 1000 --timeout 60
  ./requestx-benchmark --methods GET,POST,PUT,DELETE --sizes small,large --warmup 10
  
  # Development and debugging
  ./requestx-benchmark --quick --verbose --libraries requestx
  ./requestx-benchmark --host http://localhost:3000 --methods GET --requests 10 --verbose
        """
    )
    
    # Show help if no arguments provided
    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(0)
    
    # Test configuration
    parser.add_argument(
        '--concurrency', 
        type=str, 
        default='1,10,100',
        help='Comma-separated concurrency levels (default: 1,10,100)'
    )
    
    parser.add_argument(
        '--requests', 
        type=int, 
        default=100,
        help='Number of requests per test (default: 100)'
    )
    
    parser.add_argument(
        '--methods', 
        type=str, 
        default='GET,POST,PUT,DELETE,HEAD,OPTIONS,PATCH',
        help='Comma-separated HTTP methods to test (default: all)'
    )
    
    parser.add_argument(
        '--sizes', 
        type=str, 
        default='small,medium,large',
        help='Comma-separated request sizes to test (default: small,medium,large)'
    )
    
    parser.add_argument(
        '--timeout', 
        type=float, 
        default=300.0,
        help='Request timeout in seconds (default: 300.0)'
    )
    
    parser.add_argument(
        '--warmup', 
        type=int, 
        default=5,
        help='Number of warmup requests (default: 5)'
    )
    
    # Quick test mode
    parser.add_argument(
        '--quick', 
        action='store_true',
        help='Run quick benchmarks (fewer requests and concurrency levels)'
    )
    
    # Output configuration
    parser.add_argument(
        '--output-dir', 
        type=str, 
        default='.',
        help='Output directory for results (default: current directory)'
    )
    
    parser.add_argument(
        '--no-csv', 
        action='store_true',
        help='Disable CSV output'
    )
    
    parser.add_argument(
        '--no-json', 
        action='store_true',
        help='Disable JSON output'
    )
    
    # OpenTelemetry configuration
    parser.add_argument(
        '--grafana-cloud', 
        action='store_true',
        help='Export results to Grafana Cloud (requires GRAFANA_INSTANCE_ID and GRAFANA_API_KEY env vars)'
    )
    
    parser.add_argument(
        '--otlp-endpoint', 
        type=str,
        help='Custom OTLP endpoint for exporting metrics'
    )
    
    parser.add_argument(
        '--otlp-headers', 
        type=str,
        help='Custom OTLP headers in format "key1=value1,key2=value2"'
    )
    
    # Library selection
    parser.add_argument(
        '--libraries', 
        type=str, 
        default='requestx,requests,httpx,aiohttp',
        help='Comma-separated libraries to test (default: all available)'
    )
    
    # Host configuration
    parser.add_argument(
        '--host',
        type=str,
        default='http://localhost:8080',
        help='Base URL for the test server (default: http://localhost:8080)'
    )
    
    # Verbose output
    parser.add_argument(
        '--verbose', '-v', 
        action='store_true',
        help='Enable verbose output'
    )
    
    return parser.parse_args()


def create_config_from_args(args) -> BenchmarkConfig:
    """Create BenchmarkConfig from command line arguments."""
    
    # Parse libraries
    libraries = [x.strip().lower() for x in args.libraries.split(',')]
    
    # Parse HTTP methods to endpoints
    endpoints = []
    for method in args.methods.split(','):
        method = method.strip().upper()
        if method == 'GET':
            endpoints.append('/get')
        elif method == 'POST':
            endpoints.append('/post')
        elif method == 'PUT':
            endpoints.append('/put')
        elif method == 'DELETE':
            endpoints.append('/delete')
        else:
            endpoints.append(f'/{method.lower()}')
    
    # Quick mode adjustments
    if args.quick:
        concurrent_requests = 10
        num_requests = 20
        warmup_requests = 2
    else:
        concurrent_requests = int(args.concurrency.split(',')[0])  # Use first concurrency level
        num_requests = args.requests
        warmup_requests = args.warmup
    
    return BenchmarkConfig(
        num_requests=num_requests,
        concurrent_requests=concurrent_requests,
        timeout=args.timeout,
        warmup_requests=warmup_requests,
        libraries=libraries,
        endpoints=endpoints
    )


def filter_libraries(runner: BenchmarkRunner, libraries: str) -> None:
    """Filter benchmarkers based on requested libraries."""
    requested = [lib.strip().lower() for lib in libraries.split(',')]
    
    # Remove unwanted benchmarkers
    to_remove = []
    for lib_name in runner.benchmarkers.keys():
        if lib_name not in requested:
            to_remove.append(lib_name)
    
    for lib_name in to_remove:
        del runner.benchmarkers[lib_name]


def generate_report(results: List[BenchmarkResult], output_dir: str) -> None:
    """Generate a comprehensive benchmark report."""
    
    if not results:
        return
    
    print("\n" + "="*50)
    print("BENCHMARK REPORT")
    print("="*50)
    
    # Group by library
    library_stats = {}
    for result in results:
        lib = result.library
        if lib not in library_stats:
            library_stats[lib] = {
                'total_tests': 0,
                'avg_rps': 0,
                'avg_response_time': 0,
                'avg_memory': 0,
                'total_errors': 0,
                'total_requests': 0
            }
        
        stats = library_stats[lib]
        stats['total_tests'] += 1
        stats['avg_rps'] += result.requests_per_second
        stats['avg_response_time'] += result.average_response_time
        stats['avg_memory'] += result.memory_usage_mb
        stats['total_errors'] += result.failed_requests
        stats['total_requests'] += result.total_requests
    
    # Calculate averages
    for lib, stats in library_stats.items():
        if stats['total_tests'] > 0:
            stats['avg_rps'] /= stats['total_tests']
            stats['avg_response_time'] /= stats['total_tests']
            stats['avg_memory'] /= stats['total_tests']
    
    # Print summary
    print("\nSUMMARY:")
    print(f"{'Library':<15} {'Avg RPS':<10} {'Avg RT (ms)':<12} {'Memory (MB)':<12} {'Error Rate':<10}")
    print("-" * 70)
    
    for lib, stats in sorted(library_stats.items()):
        error_rate = (stats['total_errors'] / stats['total_requests'] * 100) if stats['total_requests'] > 0 else 0
        print(f"{lib:<15} {stats['avg_rps']:<10.2f} {stats['avg_response_time']*1000:<12.2f} {stats['avg_memory']:<12.2f} {error_rate:<10.2f}%")
    
    # Best performers
    print("\nBEST PERFORMERS:")
    
    best_rps = max(results, key=lambda r: r.requests_per_second)
    print(f"Highest RPS: {best_rps.library} - {best_rps.requests_per_second:.2f} RPS")
    
    best_latency = min(results, key=lambda r: r.average_response_time)
    print(f"Lowest Latency: {best_latency.library} - {best_latency.average_response_time*1000:.2f}ms")
    
    if hasattr(results[0], 'memory_usage_mb'):
        best_memory = min(results, key=lambda r: r.memory_usage_mb)
        print(f"Lowest Memory: {best_memory.library} - {best_memory.memory_usage_mb:.2f}MB")
    
    report_file = os.path.join(output_dir, f"benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md")
    
    with open(report_file, 'w') as f:
        f.write("# RequestX Benchmark Report\n\n")
        f.write(f"Generated: {datetime.now().isoformat()}\n\n")
        
        # Summary statistics
        f.write("## Summary\n\n")
        
        # Group by library
        library_stats = {}
        for result in results:
            lib = result.library
            if lib not in library_stats:
                library_stats[lib] = {
                    'total_tests': 0,
                    'avg_rps': 0,
                    'avg_response_time': 0,
                    'avg_memory': 0,
                    'total_errors': 0,
                    'total_requests': 0
                }
            
            stats = library_stats[lib]
            stats['total_tests'] += 1
            stats['avg_rps'] += result.requests_per_second
            stats['avg_response_time'] += result.average_response_time
            stats['avg_memory'] += result.memory_usage_mb
            stats['total_errors'] += result.failed_requests
            stats['total_requests'] += result.total_requests
        
        # Calculate averages
        for lib, stats in library_stats.items():
            if stats['total_tests'] > 0:
                stats['avg_rps'] /= stats['total_tests']
                stats['avg_response_time'] /= stats['total_tests']
                stats['avg_memory'] /= stats['total_tests']
        
        # Write summary table
        f.write("| Library | Avg RPS | Avg Response Time (ms) | Avg Memory (MB) | Error Rate (%) |\n")
        f.write("|---------|---------|------------------------|-----------------|----------------|\n")
        
        for lib, stats in sorted(library_stats.items()):
            error_rate = (stats['total_errors'] / stats['total_requests'] * 100) if stats['total_requests'] > 0 else 0
            f.write(f"| {lib} | {stats['avg_rps']:.2f} | {stats['avg_response_time']*1000:.2f} | {stats['avg_memory']:.2f} | {error_rate:.2f} |\n")
        
        # Best performers
        f.write("\n## Best Performers\n\n")
        
        best_rps = max(results, key=lambda r: r.requests_per_second)
        f.write(f"**Highest RPS:** {best_rps.library} - {best_rps.requests_per_second:.2f} RPS\n")
        
        best_latency = min(results, key=lambda r: r.average_response_time)
        f.write(f"**Lowest Latency:** {best_latency.library} - {best_latency.average_response_time*1000:.2f}ms\n")
        
        best_memory = min(results, key=lambda r: r.memory_usage_mb)
        f.write(f"**Lowest Memory:** {best_memory.library} - {best_memory.memory_usage_mb:.2f}MB\n")
        
        # Detailed results
        f.write("\n## Detailed Results\n\n")
        f.write("| Library | Method | Concurrency | RPS | Response Time (ms) | Memory (MB) | Success Rate (%) |\n")
        f.write("|---------|--------|-------------|-----|-------------------|-------------|------------------|\n")
        
        for result in sorted(results, key=lambda r: (r.library, r.method, r.concurrency)):
            success_rate = (result.successful_requests / result.total_requests * 100) if result.total_requests > 0 else 0
            f.write(f"| {result.library} | {result.method} | {result.concurrency} | "
                   f"{result.requests_per_second:.2f} | {result.average_response_time*1000:.2f} | "
                   f"{result.memory_usage_mb:.2f} | {success_rate:.2f} |\n")
    
    print(f"Detailed report saved to: {report_file}")


def main():
    """Main function to run benchmarks."""
    args = parse_arguments()
    
    # Parse concurrency levels
    if args.quick:
        concurrency_levels = [10]
    else:
        concurrency_levels = [int(x.strip()) for x in args.concurrency.split(',')]
    
    # Create base configuration
    config = create_config_from_args(args)
    
    if args.verbose:
        print(f"Configuration:")
        print(f"  Concurrent requests: {', '.join(map(str, concurrency_levels))}")
        print(f"  Requests per test: {config.num_requests}")
        print(f"  Libraries: {', '.join(config.libraries)}")
        print(f"  Endpoints: {', '.join(config.endpoints)}")
        print(f"  Timeout: {config.timeout}s")
        print(f"  Warmup requests: {config.warmup_requests}")
        print()
    
    # Initialize benchmark runner
    runner = BenchmarkRunner(config)
    
    # Base URL for testing
    base_url = args.host.rstrip('/')
    
    # Create benchmarkers for each library
    benchmarkers = {}
    for library in config.libraries:
        if library == 'requestx':
            benchmarkers[library] = RequestXBenchmarker()
        elif library == 'requests':
            try:
                import requests
                benchmarkers[library] = RequestsBenchmarker()
            except ImportError:
                print(f"Warning: Library '{library}' not installed. Skipping.")
                continue
        elif library == 'httpx':
            try:
                import httpx
                benchmarkers[library] = HttpxBenchmarker()
            except ImportError:
                print(f"Warning: Library '{library}' not installed. Skipping.")
                continue
        elif library == 'aiohttp':
            try:
                import aiohttp
                benchmarkers[library] = AiohttpBenchmarker()
            except ImportError:
                print(f"Warning: Library '{library}' not installed. Skipping.")
                continue
        else:
            print(f"Warning: Library '{library}' not supported yet. Skipping.")
            continue
    
    if not benchmarkers:
        print("Error: No supported libraries found.")
        return
    
    # Run benchmarks
    print("Running benchmarks...")
    all_results = []
    
    for library, benchmarker in benchmarkers.items():
        print(f"\nBenchmarking {library}...")
        
        for endpoint in config.endpoints:
            url = f"{base_url}{endpoint}"
            method = endpoint.upper().replace('/', '') if endpoint != '/' else 'GET'
            
            for concurrency in concurrency_levels:
                try:
                    print(f"  Testing {method} {endpoint} (concurrency: {concurrency})...")
                    
                    # Create config for this specific concurrency level
                    current_config = BenchmarkConfig(
                        num_requests=config.num_requests,
                        concurrent_requests=concurrency,
                        timeout=config.timeout,
                        warmup_requests=config.warmup_requests,
                        libraries=config.libraries,
                        endpoints=config.endpoints
                    )
                    
                    # Create runner with current config
                    current_runner = BenchmarkRunner(current_config)
                    
                    # Use profiler context to monitor performance
                    with profile_context(cpu=True, memory=True, request=True, detailed=True) as profiler_metrics:
                        result = current_runner.run_benchmark(benchmarker, url, method)
                    
                        # Enhance result with profiler metrics
                        if hasattr(result, 'cpu_usage_percent'):
                            result.cpu_usage_percent = profiler_metrics.cpu_usage_percent
                        if hasattr(result, 'memory_usage_mb'):
                            result.memory_usage_mb = profiler_metrics.memory_usage_mb
                        if hasattr(result, 'peak_memory_mb'):
                            result.peak_memory_mb = profiler_metrics.peak_memory_mb
                        
                        all_results.append(result)
                        
                        if args.verbose:
                            print(f"    RPS: {result.requests_per_second:.2f}")
                            print(f"    Avg Response Time: {result.average_response_time:.2f}ms")
                            print(f"    Error Rate: {result.error_rate:.2f}%")
                            print(f"    CPU Usage: {profiler_metrics.cpu_usage_percent:.2f}%")
                            print(f"    Memory Usage: {profiler_metrics.memory_usage_mb:.2f}MB")
                    
                except Exception as e:
                    print(f"    Error: {e}")
                    continue
    
    # Save results
    if all_results:
        # Generate timestamp for filenames
        timestamp = datetime.now().strftime("%Y-%m-%d-%H:%M:%S")
        
        # Save to JSON
        if not args.no_json:
            json_file = os.path.join(args.output_dir, f'benchmark_results_{timestamp}.json')
            os.makedirs(args.output_dir, exist_ok=True)
            
            with open(json_file, 'w') as f:
                json.dump([result.to_dict() for result in all_results], f, indent=2)
            print(f"\nResults saved to {json_file}")
        
        # Save to CSV
        if not args.no_csv:
            csv_file = os.path.join(args.output_dir, f'benchmark_results_{timestamp}.csv')
            os.makedirs(args.output_dir, exist_ok=True)
            
            import csv
            with open(csv_file, 'w', newline='') as f:
                if all_results:
                    writer = csv.DictWriter(f, fieldnames=all_results[0].to_dict().keys())
                    writer.writeheader()
                    for result in all_results:
                        writer.writerow(result.to_dict())
            print(f"Results saved to {csv_file}")
        
        # Generate report
        if args.verbose:
            print("\n" + "="*50)
            print("BENCHMARK REPORT")
            print("="*50)
            generate_report(all_results, args.output_dir)
        
        # Export to external systems
        if args.grafana_cloud:
            try:
                instance_id = os.getenv('GRAFANA_INSTANCE_ID')
                api_key = os.getenv('GRAFANA_API_KEY')
                region = os.getenv('GRAFANA_REGION', 'us-central1')
                
                if not instance_id or not api_key:
                    print("Error: GRAFANA_INSTANCE_ID and GRAFANA_API_KEY environment variables are required")
                else:
                    # Convert results to dict format
                    results_dict = [result.to_dict() for result in all_results]
                    export_to_grafana_cloud(results_dict, instance_id, api_key, region)
            except Exception as e:
                print(f"Failed to export to Grafana Cloud: {e}")
        
        if args.otlp_endpoint:
            try:
                # Parse headers if provided
                headers = None
                if args.otlp_headers:
                    try:
                        headers = json.loads(args.otlp_headers)
                    except json.JSONDecodeError:
                        print("Warning: Invalid JSON format for OTLP headers, ignoring")
                
                # Convert results to dict format
                results_dict = [result.to_dict() for result in all_results]
                export_to_otlp(results_dict, args.otlp_endpoint, headers)
            except Exception as e:
                print(f"Failed to export to OTLP endpoint: {e}")
    
    else:
        print("No benchmark results to save.")


if __name__ == "__main__":
    sys.exit(main())